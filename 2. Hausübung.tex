\documentclass[a4paper]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[ngerman]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{parskip}

% parskip
\setlength{\parskip}{1em}
\setlength{\parindent}{0pt}

% sections
\renewcommand{\thesection}{Aufgabe \arabic{section}}
\renewcommand{\thesubsection}{\alph{subsection})}
\renewcommand{\thesubsubsection}{\roman{subsubsection})}

% abbrevs
\newcommand{\dx}{\frac{\mathrm{d}}{\mathrm{d}x}}
\newcommand{\dxx}{\frac{\mathrm{d}^2}{\mathrm{d}x^2}}

\title{II. Hausübung „Wahrscheinlichkeiten“}
\author{Henrik Kröger}
\date{\today}

\begin{document}
\maketitle

\section{(Erzeugende Funktion)}
Die (gewöhnliche) erzeugende Funktion
einer diskreten Wahrscheinlichkeitsverteilung $p(n)$
für eine Zufallsvariablen $n\in\mathbb{N}$ ist definiert durch
\begin{equation}
    g_p(x) := \sum_{n=0}^{\infty} x^n p(n) \label{eq:erzeugende} \,.
\end{equation}

\subsection{Erwartungswert und Varianz durch $g(x)$ ausgedrückt}
\subsubsection{Erwartungswert}
Der Erwartungswert ist allgemein definiert als
\begin{equation}
    \langle p \rangle := \sum_{n=0}^{\infty} n \, p(n) \,.
\end{equation}
Wir vergleichen die Terme des Erwartungswertes mit denen der Erzeugenden Funktion:
\begin{align*}
    \langle p \rangle &= 0\;\;\,p(0)   + 1\;\;\,p(1) + 2\;\;\,p(2) + 3\;\;\,p(3) + 4\;\;\,p(4) + \ldots \\
    g_p(x)            &= x^0\,p(0) + x^1\,p(1) + x^2\,p(2) + x^3\,p(3) + x^4\,p(4) + \ldots
\end{align*}
$\langle p \rangle$ hat offenbar Ähnlichkeit mit der ersten Ableitung $g_p'(x)$ von $g(x)$:
\begin{align*}
    \langle p \rangle &= 0\,p(0) + 1\,p(1) + 2\;\;\,p(2) + 3\;\;\;\,p(3) + 4\;\;\;\,p(4) + \ldots \\
    g_p'(x)           &= 0\,p(0) + 1\,p(1) + 2x\,p(2) +    3x^2\,p(3)    + 4x^3\,p(4) + \ldots
\end{align*}
Es stören noch die $x$ in $g_p'(x)$, die aber durch Einsetzen von $x=1$ verschwinden:
\begin{align*}
    \langle p \rangle &= 0\,p(0) + 1\,p(1) + 2\,p(2) + 3\,p(3) + 4\,p(4) + \ldots \\
    g_p'(1)           &= 0\,p(0) + 1\,p(1) + 2\,p(2) + 3\,p(3) + 4\,p(4) + \ldots \\
\end{align*}

Nach dem dies zur richtigen Idee geführt hat,
kann dies noch in besserer Schreibweise bewiesen werden:
\begin{equation}
    g_p'(1) = \dx \sum_{n=0}^\infty x^n \, p(n) \Big|_{x=1}
            =     \sum_{n=0}^\infty n x^{n-1} \, p(n) \Big|_{x=1}
            =     \sum_{n=0}^\infty n \, p(n)
            = \langle p \rangle
\end{equation}


\subsubsection{Varianz}
Die Varianz ist nach dem Satz von König-Huygens
\begin{equation}
    \sigma_p^2 := \langle p^2 \rangle - \langle p \rangle^2 \,.
\end{equation}
$\langle p \rangle^2$ ist schon als $g_p'(1)^2$ bekannt,
$\langle p^2 \rangle$ aber ist:
\begin{equation}
    \langle p^2 \rangle = \sum_{n=0}^\infty n^2 p(n)
\end{equation}

Hat es vielleicht diesmal etwas mit der zweiten Ableitung zu tun?
$n^2$ deutet darauf hin. Wir betrachten die zweite Ableitung der Erzeugenden
an der Stelle $x=1$:
\begin{align*}
    g_p''(1) &= \dxx \sum_{n=0}^\infty x^n \, p(n) \Big|_{x=1}
             = \sum_{n=0}^\infty n(n-1) x^{n-2} \, p(n) \Big|_{x=1} \\
             &= \sum_{n=0}^\infty (n^2 - n) \, p(n)
             = \sum_{n=0}^\infty n^2 p(n) - \sum_{n=0}^\infty n p(n)
             = \langle p^2 \rangle - \langle p \rangle
\end{align*}
Das ist schon \emph{fast} die Definition der Varianz.
Lediglich $\langle p \rangle$ sollte $\langle p \rangle^2$ sein:
\begin{align*}
    g''(1) &= \langle p^2 \rangle - \langle p \rangle \\
    g''(1) + \langle p \rangle - \langle p \rangle^2  &= \langle p^2 \rangle - \langle p \rangle + \langle p \rangle - \langle p \rangle^2 = \langle p^2 \rangle - \langle p \rangle^2 = \sigma_p^2 \\
    g''(1) + g'(1) - g'(1)^2 &= \sigma_p^2
\end{align*}

Wir haben also erhalten, dass:
{\boldmath
\begin{align}
    \langle p \rangle &= g'(1) \label{eq:erwartungswert} \\
    \sigma_p^2 &= g''(1) + g'(1) - g'(1)^2 \label{eq:varianz}
\end{align}}

\newpage
\subsection{Erzeugende Funktionen der Poisson- und der Binomialverteilung}
\subsubsection{Poissonverteilung}
Die Poissonverteilung ist definiert als:
\begin{equation}
    P_\lambda (k) = \frac{\lambda^k}{k!}\, \mathrm{e}^{-\lambda}
\end{equation}
Unter Benutzung der Reihenentwicklung für die e-Funktion erhalten wir
für die erzeugende Funktion der Poissonverteilung:
\begin{equation}
    g_{P_\lambda}(x) = \sum_{k=0}^\infty x^k \frac{\lambda^k}{k!} \mathrm{e}^{-\lambda}
                     = \mathrm{e}^{-\lambda} \sum_{k=0}^\infty \frac{(x\lambda)^k}{k!}
                     = \mathrm{e}^{-\lambda} \mathrm{e}^{x\lambda}
                     = \mathrm{e}^{\lambda (x-1)}
\end{equation}

Nach (\ref{eq:erwartungswert}) ergibt sich für den Erwartungswert:
\begin{equation}
    \langle P_\lambda \rangle = g'_{P_\lambda}(1)
    = \dx \mathrm{e}^{\lambda (x-1)} \Big|_{x=1}
    = \lambda \; \mathrm{e}^{\lambda (x-1)} \Big|_{x=1}
    = \lambda
\end{equation}

Für die Varianz berechnen wir zuerst $g_{P_\lambda}''(1)$:
\begin{equation}
    g_{P_\lambda}''(1) = \lambda^2 \mathrm{e}^{\lambda (x-1)} \Big|_{x=1}
                       = \lambda^2
\end{equation}
Einsetzen in die Formel (\ref{eq:varianz}) ergibt dann:
\begin{align*}
    \sigma_{P_\lambda}^2 &= g''(1) + g'(1) - g'(1)^2 \\
                         &= \lambda^2 + \lambda - \lambda^2 = \lambda
\end{align*}

\subsubsection{Binomialverteilung}
Die Binomialverteilung ist defniert als
\begin{equation}
    B_{n,p}(k) = \binom{n}{k} p^k (1-p)^{n-k}   \qquad \text{mit}\; k=0,\ldots,n
\end{equation}
Für die Erzeugende Funktion $g_{B_{n,p}}$ lässt sich nach (\ref{eq:erzeugende}) aufschreiben:
\begin{equation}
    g_{B_{n,p}} = \sum_{k=0}^n \binom{n}{k} (xp)^k (1-p)^{n-k}
\end{equation}
Mithilfe des Binomischen Lehrsatzes
\begin{equation}
    \sum_{k=0}^n \binom{n}{k} x^{n-k} y^k  = (x+y)^n
\end{equation}
folgt über die einfache Zuordnung $x=1-p$ und $y=xp$, dass
\begin{equation}
    g_{B_{n,p}}(x) = \sum_{k=0}^n \binom{n}{k} (xp)^k (1-p)^{n-k} = (1-p+xp)^n
\end{equation}

Nach (\ref{eq:erwartungswert}) ergibt sich dann für den Erwartungswert der Binomialverteilung:
\begin{equation}
    \langle B_{n,p} \rangle = g'_{B_{n,p}}(1)
    = np(1-p+xp)^{n-1} \Big|_{x=1}
    = np 1^{n-1} = np
\end{equation}

Für die Varianz berechnen wir zuerst $g_{B_{n,p}}''(1)$:
\begin{equation}
    g_{P_\lambda}''(1) = np(n-1)p(1-p+xp)^{n-2} \Big|_{x=1}
                       = np(n-1)p = n^2 p^2 - n p^2
\end{equation}
Einsetzen in die Formel (\ref{eq:varianz}) ergibt dann
für die Varianz der Binomialverteilung:
\begin{align*}
    \sigma_{B_{n,p}}^2 &= g''(1) + g'(1) - g'(1)^2 \\
                       &= n^2 p^2 - n p^2 + np - n^2 p^2
                       = np-np^2 = np(1-p)
\end{align*}


\section{Grenzwert der Binomialverteilung}
Bestimmen Sie den Grenzwert der Binomial-Verteilung
\begin{equation}
    B(n) = \binom{m}{n} p^n (1-p)^{m-n} \label{eq:binom}
\end{equation}
für $m\to\infty$ und $\langle B \rangle = m \cdot p$ konstant.

Die Nebenbedingung
\begin{equation}
    \lim_{m\to\infty} m \cdot p = \langle B \rangle \label{eq:nebenbedingung}
\end{equation}
impliziert, dass
\begin{equation}
    \lim_{m\to\infty} p = \lim_{m\to\infty} \frac{\langle B \rangle}{m} = 0
\end{equation}
Wir sollten also $p$ in der Binomialverteilung (\ref{eq:binom}) durch (\ref{eq:nebenbedingung}) ersetzen

Mit der Definition des Binomialkoeffizienten
\begin{equation}
    \binom{m}{n} = \frac{m!}{(m-n)!\;n!}
\end{equation}
und der Nebenbedingung (\ref{eq:nebenbedingung}) lässt sich
die Binomialverteilung schreiben als
\begin{equation}
    B(n) = \frac{m!}{(m-n)!n!}
       \left( \frac{\langle B \rangle}{m} \right)^n
       \left( 1-\frac{\langle B \rangle}{m} \right)^{m-n}
\end{equation}

Bei Anwendung des Grenzwertes $\lim_{m\to\infty}$, kann man einen Faktor vorziehen
\begin{align*}
    \lim_{m\to\infty} B(n) = &\lim_{m\to\infty} \frac{m!}{(m-n)! \; n!}
       \left( \frac{\langle B \rangle}{m} \right)^n
       \left( 1-\frac{\langle B \rangle}{m} \right)^{m-n} \\
    = \frac{\langle B \rangle^n}{n!} &\lim_{m\to\infty}
        \frac{m!}{(m-n)! \; m^n}
        \left( 1-\frac{\langle B \rangle}{m} \right)^{m-n}
\end{align*}
und dann
\begin{equation}
    \lim_{m\to\infty} B(n) = \frac{\langle B \rangle^n}{n!}
    \lim_{m\to\infty}
        \frac{m!}{(m-n)! \; m^n}
        \left( 1-\frac{\langle B \rangle}{m} \right)^{m}
        \left( 1-\frac{\langle B \rangle}{m} \right)^{-n} \;,
\end{equation}
wobei der letzte Faktor offensichtlich gegen 1 konvergiert
und der vorletzte wegen der Grenzwert-Definition der e-Funktion gegen die e-Funktion
\begin{equation}
    \lim_{m\to\infty} \left( 1 + \frac{-\langle B \rangle}{m} \right)^m = \mathrm{e}^{-\langle B \rangle}
\end{equation}
konvergiert. Damit bleibt:
\begin{equation}
    \lim_{m\to\infty} B(n) = \frac{\langle B \rangle^n}{n!}
    \mathrm{e}^{-\langle B \rangle}
    \lim_{m\to\infty}
        \frac{m!}{(m-n)! \; m^n}
\end{equation}
Im verbleibenden Faktor lässt sich schreiben
\begin{equation}
    \frac{m!}{(m-n)!} = m \cdot (m-1) \cdot (m-2) \cdots (m-n+1)
\end{equation}
Weiterhin:
\begin{equation}
    \frac{ m \cdot (m-1) \cdot (m-2) \cdots (m-n+1) }{m^n}
    = \frac{m}{m} \cdot \frac{m-1}{m} \cdot \frac{m-2}{m} \cdots \frac{m-n+1}{m}
\end{equation}
Doch das konvergiert gegen 1.

Damit bleibt
\begin{equation}
    \lim_{m\to\infty} B(n) = \frac{\langle B \rangle^n}{n!}
    \mathrm{e}^{-\langle B \rangle}
\end{equation}
und das ist eine Poisson-Verteilung
mit einem Erwartungswert von $\langle B \rangle$.


\end{document}
